# -*- coding: utf-8 -*-
"""NLP 220 HW3 .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CFcTNqMDk1UTk6jAjuBCbOe-R9JlPJWW
"""

import pandas as pd
import numpy as np
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
import re
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_curve, precision_recall_curve, average_precision_score, f1_score, auc, make_scorer
from sklearn.preprocessing import MultiLabelBinarizer
from sklearn.neural_network import MLPClassifier
from sklearn.svm import SVC, LinearSVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.multiclass import OneVsRestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import RandomizedSearchCV
import argparse
import sys
from datetime import datetime
import time

def parse_args():
    parser = argparse.ArgumentParser(description='Multi-label Classification of arXiv Papers')
    parser.add_argument('--data', type=str, required=True, help='Path to input JSON file')
    parser.add_argument('--output', type=str, required=True, help='Path to output results file')
    return parser.parse_args()


def write_results(output_file, train_results, val_results, test_results, training_time):
    with open(output_file, 'w') as f:
        f.write(f"arXiv Paper Classification Results\n")
        f.write(f"Run at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")

        f.write("Training Set Results:\n")
        f.write(f"Accuracy: {train_results['accuracy']:.4f}\n")
        f.write("Classification Report:\n")
        f.write(f"{train_results['report']}\n\n")

        f.write("Validation Set Results:\n")
        f.write(f"Accuracy: {val_results['accuracy']:.4f}\n")
        f.write("Classification Report:\n")
        f.write(f"{val_results['report']}\n\n")

        f.write("Test Set Results:\n")
        f.write(f"Accuracy: {test_results['accuracy']:.4f}\n")
        f.write("Classification Report:\n")
        f.write(f"{test_results['report']}\n\n")

        f.write(f"Training Time: {training_time:.3f} seconds\n")

def main():

    args = parse_args()
    dataset = pd.read_json(args.data)
    dataset.head()

    dataset.shape

    print(dataset.dtypes)

    def preprocess_text(text):
        nltk.download('wordnet', quiet=True)
        nltk.download('stopwords', quiet=True)

        lemmatizer = WordNetLemmatizer()
        stop_words = set(stopwords.words('english'))

        # Convert to lowercase and remove special characters
        text = re.sub(r'[^a-zA-Z\s]', '', text.lower())

        # Tokenize, remove stopwords and lemmatize
        words = [lemmatizer.lemmatize(word) for word in text.split()
                 if word not in stop_words and len(word) > 2]

        return ' '.join(words)

    dataset['improved_summaries'] = dataset['summaries'].apply(preprocess_text)

    # Feature 1 - Length of summaries
    dataset['improved_summaries'] = dataset['improved_summaries'].fillna('')
    dataset['Description_Length'] = dataset['improved_summaries'].apply(lambda x: len(x.split()))
    dataset.head()

    nltk.download('stopwords')
    stop_words = list(set(stopwords.words('english')))

    vectorizer = CountVectorizer(max_features=10, stop_words=stop_words)
    X = vectorizer.fit_transform(dataset['improved_summaries'])
    keywords = vectorizer.get_feature_names_out()
    dataset['Keywords'] = dataset['improved_summaries'].apply(
        lambda x: [word for word in keywords if word in x.lower()])

    dataset.head()

    # Feature 2 - Bag of Words
    vectorizer = CountVectorizer(max_features=2000, ngram_range=(1, 2), min_df=3, max_df=0.95)
    word_freq_matrix = vectorizer.fit_transform(dataset['improved_summaries'])
    word_freq_df = pd.DataFrame(word_freq_matrix.toarray(), columns=vectorizer.get_feature_names_out())
    word_freq_df.head()

    # Feature 3 - TFIDF
    tfidf_vectorizer = TfidfVectorizer(max_features=1000, ngram_range=(1, 2), min_df=3, max_df=0.95, use_idf=True, smooth_idf=True)
    tfidf_matrix = tfidf_vectorizer.fit_transform(dataset['improved_summaries'])
    tfidf_df = pd.DataFrame(tfidf_matrix.toarray(),
                            columns=tfidf_vectorizer.get_feature_names_out())
    tfidf_df.head()

    # Split terms and get all unique values
    all_terms = []
    for term_list in dataset['terms']:
        all_terms.extend(term_list)  # Directly extend with the list of terms

    # Count frequency of each term
    term_counts = pd.Series(all_terms).value_counts()

    # Create bar plot
    plt.figure(figsize=(10, 6))
    sns.barplot(x=term_counts.index, y=term_counts.values)
    plt.title('Distribution of Terms')
    plt.xlabel('Terms')
    plt.ylabel('Count')
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.show()

    print(term_counts)

    # Feature Engineering 1
    X_f1 = pd.concat([dataset[['Description_Length']], word_freq_df, tfidf_df], axis=1)
    y = dataset['terms']

    X_train1, X_temp1, y_train1, y_temp1 = train_test_split(X_f1, y, test_size=0.30, random_state=1234)
    X_val1, X_test1, y_val1, y_test1 = train_test_split(X_temp1, y_temp1, test_size=0.50, random_state=1234)

    start_time = time.time()
    # Random Forest - 1
    clf = MultiLabelBinarizer()
    y_train_bin = clf.fit_transform(y_train1)
    y_val_bin = clf.transform(y_val1)

    classifierRF1 = OneVsRestClassifier(RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 42))
    classifierRF1.fit(X_train1, y_train_bin)
    y_pred_bin = classifierRF1.predict(X_val1)

    # Evaluate the model
    accuracy = accuracy_score(y_val_bin, y_pred_bin)
    print(f'Accuracy: {accuracy:.2f}')
    print(classification_report(y_val_bin, y_pred_bin))

    # Best Model on Test Set
    clf = MultiLabelBinarizer()
    y_train_bin = clf.fit_transform(y_train1)
    y_test_bin = clf.transform(y_test1)
    y_val_bin = clf.transform(y_val1)

    classifierRF1 = OneVsRestClassifier(RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 42))
    classifierRF1.fit(X_train1, y_train_bin)

    # Make predictions on test set
    y_pred_test = classifierRF1.predict(X_test1)
    print("\nTest Set Performance:")
    print("Accuracy:", accuracy_score(y_test_bin, y_pred_test))
    print("\nClassification Report:")
    print(classification_report(y_test_bin, y_pred_test))

    training_time = time.time() - start_time

    train_results = {
        'accuracy': accuracy_score(y_train_bin, classifierRF1.predict(X_train1)),
        'report': classification_report(y_train_bin, classifierRF1.predict(X_train1))
    }

    val_results = {
        'accuracy': accuracy_score(y_val_bin, classifierRF1.predict(X_val1)),
        'report': classification_report(y_val_bin, classifierRF1.predict(X_val1))
    }

    test_results = {
        'accuracy': accuracy_score(y_test_bin, y_pred_test),
        'report': classification_report(y_test_bin, y_pred_test)
    }

    write_results(args.output, train_results, val_results, test_results, training_time)

if __name__ == "__main__":
    main()